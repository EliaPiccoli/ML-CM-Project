\documentclass[12pt]{article}
\usepackage{graphicx} % for inserting images
\usepackage[utf8]{inputenc} % for good practice
\usepackage{amsmath} % for equations
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{setspace}
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\Lagr}{\mathcal{L}}

\begin{document}
    \section{Knapsack Problem}
    After the previous formalization the optimization problem is the following
	\begin{equation}\label{eq:7}
	    \begin{aligned}
	    \max_{\beta_i} &- \frac{1}{2}\sum_i \sum_j \beta_i \beta_j K(x_i,x_j) \\
		&- \varepsilon\sum_i\abs{\beta_i}\\
		&+ \sum_i y_i\beta_i\\
		&With\;the\;constraints\qquad
        \begin{cases}
            \sum_i \beta_i = 0 \\
            \;\beta_i\in[-C,\;C] 
        \end{cases}
        \end{aligned}
	\end{equation}
	The problem of maximization can be substituted by a minimization obtaining a formulation that recalls the one of a Convex Separable Knapsack problem.
	\begin{equation}\label{eq:8}
	    \begin{aligned}
	    \min_{\beta_i} \quad & \frac{1}{2}\sum_i \sum_j \beta_i \beta_j K(x_i,x_j) \\
		&+ \varepsilon\sum_i\abs{\beta_i}\\
		&- \sum_i y_i\beta_i\\
		&With\;the\;constraints\qquad
        \begin{cases}
            \sum_i \beta_i = 0 \\
            \;\beta_i\in[-C,\;C] 
        \end{cases}
        \end{aligned}
	\end{equation}
	We define the Lagrangian Relaxation:
	\begin{equation}\label{eq:9}
	    \begin{aligned}
	    \Lagr(\mu) =  \quad & \frac{1}{2}\sum_i \sum_j \beta_i \beta_j K(x_i,x_j) \\
		&+ \varepsilon\sum_i\abs{\beta_i}\\
		&- \sum_i y_i\beta_i\\
		&+ \mu(\sum_i \beta_i - 0)\\
		&With\;the\;constraints\qquad
        \;\beta_i\in[-C,\;C] 
		\end{aligned}
	\end{equation}
    The objective is to minimize $\beta$ and to do so we calculate $\frac{\partial\Lagr}{\partial\beta_i} = 0$
    \begin{equation}\label{eq:10}
        \frac{\partial\Lagr}{\partial\beta_i} \longrightarrow \beta_i K(x_i,x_i) 
        + \varepsilon sgn(\beta_i)
        - y_i
        + \mu
    \end{equation}
    At this point we can define the two breakpoints for each $\beta_i$, which represent the upper and lower bound of the Lagrangian multiplier $\mu$.\\
    \emph{Remark:} upper and lower bound for each $\beta_i$ is the same (-C; C).
    \begin{equation}
        \begin{aligned}
            &\mu_i^l = -(-C * K(x_i,x_i)) - \varepsilon sgn(-C) + y_i \\
            &\mu_i^u = -(C * K(x_i,x_i)) - \varepsilon sgn(C) + y_i \\
            &where \quad \forall_i \quad \mu_i^u < \mu_i^l
        \end{aligned}
    \end{equation}
    Each $\beta_i$ can be defined wrt to $\mu$
    \begin{equation}
        \begin{aligned}
            \beta_i(\mu) = 
            \begin{cases}
                C \qquad &if\;\mu < \mu_i^u\\
                \frac{y_i - \varepsilon sgn(\beta_i) -\mu}{K(x_i,x_i)} &if\; \mu_i^u \leq \mu \leq \mu_i^l\\
                -C &if\;\mu > \mu_i^l\\
            \end{cases}
        \end{aligned}
    \end{equation}
    At this point using for example \emph{Algorithm 3.1} of [\ref{p:1}], we can compute the value of $\mu^*$ that implies how to find the correct values of $\beta_i$ that satisfy the constraints.
    
    \pagebreak
    
    \section{How to use it}
    Now that we have given a formulation for the resolution of the Knapsack problem, the focus shifts towards how we can use it in our minimization problem that needs to be solved using a deflected subgradient method.\\
    At each step we compute the values of $\beta$ and we want to make sure that their values satify the linear and box constraints imposed by the problem.\\
    Here a pseudocode that summarize the steps.
    \begin{algorithm} \label{alg:1}
        \Large
        \While{optima criteria are not satisfied}{
	        $v \longleftarrow f(\beta)$ \\
	        $g \longleftarrow \nabla f(\beta)$ \\
	        \texttt{check if a better solution was found} \\
	        $d \longleftarrow \gamma g + (1 - \gamma)dprev$ \\
	        $stepsize \longleftarrow \frac{\psi(v-f_{ref}+\delta)}{\norm{d}^2}$ \\
	        $\beta \longleftarrow \beta - stepsize\cdot d$ \\
	        \texttt{project $\beta$ by solving the KP} \\
	    }
		\caption{Compute $\beta$}
    \end{algorithm} \\
    A probably trivial point, related to the computation of the direction, makes our approach incorrect. As we can see the current direction depends on the previous direction, but if we take a closer look at the execution flow, it does not. \\
    What algorithm \ref{alg:1} does at every iteration is a lot of calculations to find a direction, a stepsize and therefore a new $\beta$ which then gets projected in the feasible region to satisfy the constraints. The projection, as shown in \emph{algorithm 3.1} of [\ref{p:1}], is based uniquely on the use of breakpoints, created using the upper and lower bound of the $\beta_i$ variable. Therefore the actual computed values of $\beta_i$ before the projection are not taken into consideration. \\
    This leads to a deterministic computation of $\beta_i$ values which depends only on the solution of the knapsack problem, giving zero weight to the deflection applied before. We are stunned (and not in a good way): we have a clearer understanding of how deflection works and also of how a knapsack problem can be solved. We find ourselves in a \emph{saddle point} trying to figure out how to merge the two.
    
    \pagebreak
    \section{Reference}
    \begin{enumerate}
        \item
            \href{https://link.springer.com/article/10.1007/s10107-006-0050-z}
                {Breakpoint searching algorithms for the continuous quadratic knapsack problem} - Krzysztof C. Kiwiel \label{p:1}
    \end{enumerate}
\end{document}