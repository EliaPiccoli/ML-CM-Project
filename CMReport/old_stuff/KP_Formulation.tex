\documentclass[12pt]{article}
\usepackage{graphicx} % for inserting images
\usepackage[utf8]{inputenc} % for good practice
\usepackage{amsmath} % for equations
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{setspace}
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\Lagr}{\mathcal{L}}

\begin{document}
    \section{Projection formulation}
    Starting from the following formalization:
	\begin{equation}\label{eq:8}
	    \begin{aligned}
	    \min_{\beta_i} \quad & \frac{1}{2}\sum_i \sum_j \beta_i \beta_j K(x_i,x_j) \\
		&+ \varepsilon\sum_i\abs{\beta_i}\\
		&- \sum_i y_i\beta_i\\
		&With\;the\;constraints\qquad
        \begin{cases}
            \sum_i \beta_i = 0 \\
            \;\beta_i\in[-C,\;C] 
        \end{cases}
        \end{aligned}
	\end{equation}
    The optimization problem can be solved by the following steps, including the deflection as specified by the task:
    \begin{algorithm} \label{alg:1}
        \small
        \While{optima not found}{
	        $v \longleftarrow f(\beta)$ \\
	        $g \longleftarrow \nabla f(\beta)$ \\
	        \texttt{\small check if current solution is best so far (to update)} \\
	        $d \longleftarrow \gamma g + (1 - \gamma)dprev$ \\
	        $stepsize \longleftarrow \frac{\psi(v-f_{ref}+\delta)}{\norm{d}^2}$ \\
	        $\beta \longleftarrow \beta - stepsize\cdot d$ \\
	        \texttt{project $\beta$ by solving the KP} \\
	    }
		\caption{Compute $\beta$}
    \end{algorithm} 
    \newline
    Projection of line 8 is where a simple formulation can be exploited to solve efficiently a Convex Separable Knapsack Problem. The main objective of the formulation is keeping the resulting $\beta_{proj}$ vector as similar as possible to the one produced by line 7 while at the same time producing a $\beta_{proj}$ vector satisfying the constraints. Therefore:
    \begin{equation}\label{eq:9}
	    \begin{aligned}
	    \min_{\beta_{proj}} \quad &\frac{1}{2}\norm{\beta - \beta_{proj}}^2\\ 
		&With\;the\;constraints\qquad
        \begin{cases}
            \sum_i \beta_{proj}^i = 0 \\
            \;\beta_{proj}^i\in[-C,\;C] 
        \end{cases}
        \end{aligned}
	\end{equation}
	
	Which by lagrangian relaxation leads to:
	\begin{equation}\label{eq:10}
	    \begin{aligned}
	    \min_{\beta_{proj}} \quad &\frac{1}{2}\sum_i((\beta^i - \beta_{proj}^i)^2 + \mu \beta_{proj}^i)\\
		&With\;the\;constraints\qquad
        \begin{cases}
            \;\beta_{proj}^i\in[-C,\;C] 
        \end{cases}
        \end{aligned}
	\end{equation}
    
    
    \section{Alternative formulation}
    A different concept came up while discussing with each other, probably slightly more intensive on the computational side (because of the more complex constraints), but carrying the same underlying thought process. Instead of focusing on ensuring the constraints exploiting as reference the $\beta$ vector from line 7 of [\ref{alg:1}] we could ensure them by exploiting the $d$ vector from line 5. This would imply projecting the deflected direction trying to keep as small as possible the variation to it, which in the language of mathematics results in:
    \begin{equation}\label{eq:11}
	    \begin{aligned}
	    \min_{s} \quad \frac{1}{2}\norm{d - s}^2
        \end{aligned}
	\end{equation}
	
	Obviously the constraints are different in this occasion. The only constraints are on $\beta$ and since the supposedly new $\beta$ is constructed using the chosen direction (i.e. $\beta = \beta - stepsize \cdot s)$) we can test and ensure the constraints by deriving the ones relative to $s$:
	\begin{equation}
	    \begin{aligned}
        \begin{cases}
            \;s_i\in[\frac{\beta_i-C}{stepsize},\;\frac{\beta_i+C}{stepsize}]\\
            \;\sum_i s_i = \frac{\sum_i\beta_i}{stepsize}
        \end{cases}
        \end{aligned}
	\end{equation}
	Which, wrapping up by lagrangian relaxation, leads to:
	\begin{equation}\label{eq:6}
	    \begin{aligned}
	    \min_{s} \quad &\frac{1}{2}\sum_i(d_i - s_i)^2 + \mu(s_i - \frac{\sum_j\beta_j}{stepsize})\\
	    &With\;the\;constraints\qquad
	    \begin{cases}
            \;s_i\in[\frac{\beta_i-C}{stepsize},\;\frac{\beta_i+C}{stepsize}]
        \end{cases}
        \end{aligned}
	\end{equation}
	
	$stepsize$ is obtained using original direction $d$ so to avoid a recursive constraint on $s$. This is not a dramatic approximation since the objective of the minimization of \eqref{eq:11} is keeping $s$ as close as possible to $d$.
	
	\section{Analysis of the projection problems}
        Let's consider the two possible projection problems and try to correlate one with the other.
        \begin{equation}
            \begin{aligned}
                &\emph{Project the direction}\quad &&\longrightarrow \quad &&\min \norm{d-s} \\
                &\emph{Project the point} &&\longrightarrow &&\min \norm{\beta-\beta_{proj}}\\
            \end{aligned}
        \end{equation}
        Consider as starting point the second one. \\
        We can develop the two $\beta$ by its definition, as we can see at step 7 of \ref{alg:1}.
        \begin{equation}
            \begin{aligned}
                &\norm{\beta-\beta_{proj}} = \norm{\beta_{old} - stepsize \cdot d - (\beta_{old} - stepsize^\prime \cdot d^\prime )}\\\\
                &\qquad \qquad simplifying\\\\
                &\norm{\beta-\beta_{proj}} = \norm{- stepsize \cdot d + stepsize^\prime \cdot d^\prime}
            \end{aligned}
        \end{equation}
        Where $stepsize^\prime$ and $d^\prime$ represent the stepsize and direction projected into the feasible region.\\\\
        As we previously explained for \eqref{eq:6} we can approximate $stepsize^\prime$ with $stepsize$ without loss of generality.
        \begin{equation}
            \begin{aligned}
                    &\norm{-stepsize \cdot d + stepsize^\prime \cdot d^\prime} \approx \norm{stepsize(d^\prime - d)} \\\\
                    &stepsize\norm{d^\prime - d} \approx \norm{d^\prime -d}
            \end{aligned}
        \end{equation}
        So we can conclude that with reasonable assumption and with a small margin of approximation the two problems at the end of the day lead to the same solution.\\
        Obviously this differences will play a role at each step of the iteration, so the analysis of both should be interesting to maybe derive some properties or behaviours wrt solving one projection or the other.
\end{document}