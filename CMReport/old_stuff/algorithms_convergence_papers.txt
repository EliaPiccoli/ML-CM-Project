Projected Gradient -> always converge in O(1/t)
	- https://link.springer.com/content/pdf/10.1007/BF02592073.pdf
	- http://www.optimization-online.org/DB_FILE/2018/09/6825.pdf
	- https://www.researchgate.net/profile/Yakui-Huang-2/publication/275247718_On_the_rate_of_convergence_of_projected_Barzilai-Borwein_methods/links/5caf591492851c8d22e3c96f/On-the-rate-of-convergence-of-projected-Barzilai-Borwein-methods.pdf
	

	- http://www.princeton.edu/~yc5/ele522_optimization/lectures/grad_descent_constrained.pdf
	- https://angms.science/doc/CVX/CVX_PGD.pdf
	- http://niaohe.ise.illinois.edu/IE598_2016/pdf/IE598-lecture10-projected%20gradient%20descent.pdf

	Maybe read:
	- https://www.researchgate.net/publication/37594158_Convergence_of_a_gradient_projection_method
	- https://www.sciencedirect.com/science/article/pii/0041555366901145?via%3Dihub

Convex Separable Knapsack Problem -> always converge in O(n log n)
	- http://pages.di.unipi.it/frangio/abstracts.html#EJOR13
	- https://link.springer.com/article/10.1007/s10107-006-0050-z (has proof)

Deflected Subgradient -> always converge in O(1/t^2)
	- https://core.ac.uk/download/pdf/11828603.pdf (has proof)
