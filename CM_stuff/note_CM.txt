SKETCH OF WHAT WE NEED TO DO (?): https://medium.com/@univprofblog1/support-vector-regression-matlab-r-and-python-codes-all-you-have-to-do-is-preparing-data-set-1d8e4333f831#.f7oxj8jkg

CODE BUT WITH AUTOMATIC OPTIMIZER (?): https://github.com/lbugnon/SVR

ONLINE SVR e' un SVR che si aggiorna man mano che le arrivano info, non nostro caso!


PRETTY STRAIGHT FORWARD EXPLAN ON SVR = https://www.ics.uci.edu/~welling/teaching/KernelsICS273B/SVregression.pdf

SVR si prova a costruire una linea che rappresenti il mio dato di input con meno errore possibile. Questa linea è data da wx+b ed una volta assegnato un tubo di grandezza ε vogliamo che tutti i nostri valori veri (y) siano all'interno di questo tubo. Ovviamente c'è la problematica dei valori troppo distanti dalla linea, per questo introduciamo le variabili slack che ci permettono di imprimere dei constraint stretti ma allo stesso tempo "larghi" permettendo la presenza di questi valori y particolarmente distaccati dalla linea. 
L'importanza di questi valori distaccati è data dal peso dell'iperparametro C.
Il nostro obiettivo principale è dunque quello di ottimizzare il problema (che rendiamo quadratico per convenienza, elevando al quadrato w):
(PRIMAL) 1 / 2 * ||w||^2 + (C/2)*sommatoria(ξ ^ 2 + ξ' ^ 2) in verità noto che la sommatoria può contenere (ξ + ξ') (cosiddetti constraints lineari contro i primi, detti quadratici)

essendo un problema quadratico, ci troviamo nella casistica in cui la soluzione del problema primale è la stessa del problema duale, che in questa casistica è più facile da risolvere grazie alla presenza della possibilità di utilizzo dei KERNEL.

Domanda: come cazzo famo sto training? Non capisco, sembrerebbe a sto punto che dobbiamo trasformare il problema in duale, una volta che l'abbiamo in quella forma ci basta trovare le alpha tali per cui arriviamo ad un minimo (ed il minimo esiste ed è unico (?) dato che il problema che stiamo risolvendo è quadratico). Una volta che abbiamo le alpha è una cazzata trovare W e sapendo un qualsiasi punto che sia l'equivalente dei support vector per SVC è una cazzata trovare la b. A quel punto fare predizione non è altro che applicare w(trasposto)*input e il valore risultante ce lo mettiamo: esatto, su per il culo. Non è classification quindi immagino sarà da fare LMS o MEE anche qua e vedere che succ / quanto bassa è la loss (più bassa è meglio è)
